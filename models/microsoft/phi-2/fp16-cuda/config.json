{
  "model" : {
    "type": "phi2",
    "decoder" : "phi-2_decoder_fp16_opt.onnx",
    "vocab_size": 51200,
    "hidden_size": 80,
    "num_attention_heads": 32,
    "num_hidden_layers": 32,
    "num_key_value_heads": 32,
    "logits_type" : "float16",
    "kv_type" : "float16",
    "past_names" : "past_%d",
    "present_names" : "present_%d"
  },

  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0.dev0",
  "use_cache": true,
  
  "max_length": 4096,
  "min_length":0,
  "top_p":0.7,
  "temperature":0.6
}
